<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="chrome=1" />
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
        <title>Qiuyi (Richard) Zhang - Home</title>
        <link href="./css/default.css" rel="stylesheet" type="text/css" async />
	<link href="https://fonts.googleapis.com/css?family=Architects+Daughter" async rel="stylesheet" type="text/css" />
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
	});
	</script>
	<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="./">Qiuyi (Richard) Zhang &nbsp; (张秋逸)</a>
            </div>
            <div id="navigation">
                <a href="./">Home</a>
		<!-- <a href="/academics.html">Research</a> -->
                <a href="./projects.html">Projects</a>
            </div>
        </div>

        <div id="content">
            <h1>Home</h1>

            <p><span class="alignright"> <img src="./images/qiuyiz_security.jpg" width="140px"> </span></p>
<h2 id="short-bio">Short Bio</h2>
<p>I am a Staff Research Scientist at <a href="https://www.deepmind.com/">Google Deepmind</a> on the Gemini team and co-creator of <a href="https://github.com/google/vizier">OSS Vizier</a>, working on LLM posttraining and reward models, prompt/hyperparameter optimization, and theoretical machine learning. I also dabble a bit in AI alignment, counterfactuals/fairness and the intersection of <a href="https://aiandfaith.org/"> Faith and AI.</a> </p> 
		
I steward the Global Christians in AI  <a href="https://chaiglobal.ai">(CHAI)</a> community, where AI practitioners, academics, theologians, and entrepreneurs come together monthly to discuss relevant topics in the intersection of Christianity and AI. We organize talks and socials/workshops at Google and academic conferences; please <a href="https://sites.google.com/view/chai-global/">join the community</a> if you would like to learn more!
		
<p> I am grateful to have graduated with a PhD in Applied Mathematics and Computer Science at <a href="https://www.berkeley.edu/">UC Berkeley</a>, where I was fortunate to be advised under Prof. Satish Rao and Prof. Nikhil Srivastava. My <a href="./projects.html">interests</a> are in the intersection of optimization, theoretical computer science and machine learning. Previously, I graduated in the Great Class of 2014 from <a href="http://www.princeton.edu/">Princeton University</a>.</p>

<h1 id="papers">Pre-prints and Talks/Media</h1>
<div id="pub" class="separateli">
<ul>
<li><p><a href="https://chaiglobal.ai/prompting-bible">Prompting Bible (for creators)</a>   </span><br /> with <a href="https://chaiglobal.ai">CHAI</a>. 
<li><p><a href="https://docs.google.com/presentation/d/1gL4UhKuWjZ3HJFiU5JOMp6Ke313fVCtN/edit?usp=sharing&ouid=100672459849995985525&rtpof=true&sd=true">Research Insights on Redemptive Usecases</a>   </span><br /> with <a href="https://chaiglobal.ai">CHAI</a>. <br /> <strong>Missional AI 2025.</strong></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1bvF2R3DATAAHkfsvZeLLjUzr7s0epTRrdaftNP_X-oA/edit?usp=sharing&resourcekey=0-Ojb1uoZGHi7Ra51lh6Zh5A">Belief-based AI Fairness</a>   </span><br /> with <a href="https://chaiglobal.ai">CHAI</a>. <br /> <strong>CHAI Speaker Series 2023.</strong></p></li>	
<li><p><a href="https://docs.google.com/presentation/d/1tyZn6zOrpe16q0bdGDDsrHzIDor5bH4HNUx-N1rAHEo/edit?usp=sharing">Introduction to AI at Accord</a>   </span><br /> with <a href="https://chaiglobal.ai">CHAI</a>. <br /> <strong>Accord 2023.</strong></p></li>
</ul>
</div>

<h1 id="papers">AI Alignment and Society/Media</h1>
<div id="pub" class="separateli">
<ul>
<li><p><a href="">Cultural Perspectives and Expectations for Generative AI: A Global Survey Approach</a>     </span><br /> with Erin van Liemt, et al. <br /> <strong>Nature (in submission).</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2405.19534">Preference Learning Algorithms Do Not Learn Preference Rankings</a>  </span><br /> with Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Rajesh Ranganath, Kyunghyun Cho. <br /> <strong>Neurips 2024.</strong></p></li>
<li><p><a href="https://www.arxiv.org/abs/2408.01455">Ontology of Belief Diversity: A Community-Based Epistemological Approach</a>     </span><br /> with Tyler Fischella, Erin van Liemt. <br /> <strong>AIES 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2310.13018">Getting Aligned on Representational Alignment</a>   </span><br /> with multiple authors. <br /> <strong>ArXiV 2023.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2307.06513">Leveraging Contextual Counterfactuals Toward Belief Calibration</a>   </span><br /> with Michael S. Lee, Sherol Chen <br /> <strong>ICML 2023.</strong></p></li>
</ul>
</div>


<h1 id="papers">Papers</h1>
<div id="pub" class="separateli">
<ul>
<li><p><a href="">Randomization Boost KV Caching, Learning Balances Query Load: A Joint Perspective</a>  </span><br /> with Fangzhou Wu and Sandeep Silwal. <br /> <strong>ICLR 2026.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2510.08413">Prompts Generalize with Low Data: Non-vacuous Generalization Bounds for Optimizing Prompts with More Informative Priors</a>  </span><br /> with David Madras and Joshua Safyan. <br /> <strong>ICML 2025.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2505.06518">Reinforcement Learning under State and Outcome Uncertainty: A Foundational Distributional Perspective</a>  </span><br /> with Larry Preuett and Muhammad Ahmad. <br /> <strong>RLC 2025.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2502.19865">Beyond Worst-Case Dimensionality Reduction for Sparse Vectors</a>  </span><br /> with Sandeep Silwal and David Woodruff. <br /> <strong>ICLR 2025.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2411.07483">Quantifying Knowledge Distillation Using Partial Information Decomposition</a>  </span><br /> with Pasan Dissanayake, Faisal Hamman, Barproda Halder, Ilia Sucholutsky, Sanghamitra Dutta. <br /> <strong>AIStats 2025.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2411.17965">Optimized Tradeoffs for Private Prediction with Majority Ensembling</a>  </span><br /> with Shuli Jiang, Gauri Joshi. <br /> <strong>TMLR 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2307.03288">Optimal Scalarizations for Sublinear Hypervolume Regret</a>  </span><br /> Solo paper. <br /> <strong>Neurips 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2408.11527">The Vizier Gaussian Process Bandit Algorithm</a>  </span><br /> with Xingyou Song, Chansoo Lee, Emily Fertig, Tzu-Kuo Huang, Lior Belenki, Greg Kochanski, Setareh Ariafar, Srinivas Vasudevan, Sagi Perel, Daniel Golovin. <br /> <strong>ArXiv 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2410.10190">Predicting from Strings: Language Model Embeddings for Bayesian Optimization</a>  </span><br /> with Tung Nguyen, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, Xingyou Song. <br /> <strong>ArXiV 2024 (In Submission).</strong></p></li>	
<li><p><a href="https://arxiv.org/abs/2307.06513">Quantifying Spuriousness of Biased Datasets Using Partial Information Decomposition</a> </span><br /> with Barproda Halder, Faisal Hamman, Pasan Dissanayake, Ilia Sucholutsky, Sanghamitra Dutta. <br /> <strong>ICML 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2401.09278">Adaptive Regret for Bandits Made Possible: Two Queries Suffice</a>   </span><br /> with Zhou Lu, Xinyi Chen, Fred Zhang, David Woodruff, Elad Hazan. <br /> <strong>ICLR 2024.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2307.02245">Set Learning for Accurate and Calibrated Models</a>   </span><br /> with Lukas Muttenthaler, Robert A. Vandermeulen, Thomas Unterthiner, Klaus-Robert Müller. <br /> <strong>ICLR 2024.</strong></p></li>		
<li><p><a href="https://arxiv.org/abs/2311.01960">Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products</a>   </span><br /> with Tamas Sarlos, Xingyou Song, David Woodruff<br /> <strong>Neurips 2023.</strong></p></li>	
<li><p><a href="https://arxiv.org/abs/2311.04158">Computing Approximate ell_p Sensitivities</a>   </span><br /> with Swati Padmanabhan, David Woodruff. <br /> <strong>Neurips 2023.</strong></p></li>	
<li><p><a href="https://arxiv.org/abs/2304.07413">Robust Algorithms on Adaptive Inputs from Bounded Adversaries</a>   </span><br /> with Yeshwanth Cherapanamjeri, Sandeep Silwal, David Woodruff, Fred Zhang, Samson Zhou. <br /> <strong>ICLR 2023.</strong></p></li>	
<li><p><a href="https://arxiv.org/abs/2205.13320">Towards Learning Universal Hyperparameter Optimizers with Transformers</a>   </span><br /> with Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc'aurelio Ranzato, Sagi Perel, Nando de Freitas. <br /> <strong>Neurips 2022.</strong> <a href="https://ai.googleblog.com/2022/08/optformer-towards-universal.html">[Google AI Blog]</a></p></li>	
<li><p><a href="https://arxiv.org/abs/2209.15219">Optimal Query Complexities for Dynamic Trace Estimation</a>   </span><br /> with David Woodruff, Fred Zhang. <br /> <strong>Neurips 2022 (Spotlight).</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2203.04274">Leveraging Initial Hints for Free in Stochastic Linear Bandits</a>   </span><br /> with Ashok Cutkosky, Chris Dann, Abhimanyu Das. <br /> <strong>ALT 2022.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2111.00664">Optimal Sketching for Trace Estimation</a>   </span><br /> with Shuli Jiang, Hai Pham, David Woodruff. <br /> <strong>Neurips 2021 (Spotlight).</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2103.15261">One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks</a>   </span><br /> with Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal Sharan, Xin Wang. <br /> <strong>ICLR 2021.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2006.04655">Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization</a>   </span><br /> with Daniel Golovin. <br /> <strong>ICML 2020.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1911.06317">Gradientless Descent: High-Dimensional Zeroth-Order Optimization</a>   <span class="alignright"><a href="./static/gradientless.pdf">[slides]</a> </span><br /> with Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song. <br /> <strong>ICLR 2020 (Spotlight).</strong></p></li>
<li><p><a href="https://arxiv.org/abs/2002.08202">Span Recovery for Deep Neural Networks with Applications to Input Obfuscation</a>    </span><br /> with Rajesh Jayaram, David Woodruff. <br /> <strong>ICLR 2020.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1911.06958">Regularized Weighted Low Rank Approximation</a>    </span><br /> with Frank Ban, David Woodruff. <br /> <strong>Neurips 2019.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1905.04447">Solving Empirical Risk Minimization in the Current Matrix Multiplication Time</a>    </span><br /> with Zhao Song, Yin Tat Lee. <br /> <strong>COLT 2019.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1811.01121">Optimal Sequence Length Requirements for Phylogenetic Tree Reconstruction with Indels</a>    </span><br /> with Arun Ganesh. <br /> <strong>STOC 2019.</strong></p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-3-030-18174-1_12">Using INC Within Divide-and-Conquer Phylogeny Estimation</a>    </span><br /> with Thien Le, Aaron Sy, Erin Molloy, Satish Rao, Tandy Warnow. <br /> <strong>AlCoB 2019.</strong></p></li>
<li><p><a href="https://drops.dagstuhl.de/opus/volltexte/2018/9310/pdf/LIPIcs-WABI-2018-8.pdf">New Absolute Fast Converging Phylogeny Estimation Methods with Improved Scalability and Accuracy</a>    </span><br /> with Satish Rao, Tandy Warnow. <br /> <strong>WABI 2018.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1702.00458">Convergence Results for Neural Networks via Electrodynamics</a>    </span><br /> with Rina Panigrahy, Sushant Sachdeva. <br /> <strong>ITCS 2018.</strong></p></li>
<li><p><a href="https://arxiv.org/abs/1308.5170">Forbidden Directed Minors and Kelly-width</a>    </span><br /> with Shiva Kintali. <br /> <strong>Theoretical Computer Science 2017, Vol. 662.</strong></p></li>
</ul>
</div>
		
<h2 id="contact">Contact/Related</h2>
<ul>
<li>LinkedIn: richard-zhang-1b358a39</li>	
<li>Personal Email: qiuyizhang (at) gmail (dot) com</li>
</ul>
        </div>
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35711313-2', 'qiuyiz.github.io');
  ga('send', 'pageview');

</script>
    </body>
</html>
